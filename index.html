<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>bicelove</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="bicelove">
<meta property="og:url" content="http://bicelove.com/index.html">
<meta property="og:site_name" content="bicelove">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="bicelove">
  
    <link rel="alternative" href="/atom.xml" title="bicelove" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <section id="main">
  
    <article id="post-Tensorflow queue" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/03/24/Tensorflow queue/">Tensorflow Queue</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/24/Tensorflow queue/" class="article-date">
  <time datetime="2017-03-24T14:39:00.000Z" itemprop="datePublished">2017-03-24</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>队列是TensorFlow图中的一种有状态的节点，像变量一样可以被其他节点修改。具体来说，其他节点可以把新元素插入到队列后端(rear)，也可以把队列前端(front)的元素删除。</p>
<p>QueueRunners：<br>文件名队列是怎么得来的呢？起初这个队列是空的，QueueRunners本质上就是一个线程thread，负责使用会话session并不断地调用enqueue操作。Tensorflow把这个模式封装在tf.train.QueueRunner对象里面。入队列操作99%的时间都可以被忽略掉，因为这个操作是由后台负责运行。</p>
<p>RandomShuffleQueue(A queue implementation that dequeues elements in a random order)：<br>一个典型的输入结构是使用一个RandomShuffleQueue作为模型训练的输入，多个线程准备训练样本，并且把这些样本压入队列，一个训练线程执行一个训练操作，此操作会从队列中移除一个mini-batche，Session对象支持多线程并行操作。</p>
<p>但是，所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候， 队列必须能被正确地关闭。所以TensorFlow提供了两个类来帮助多线程的实现：tf.Coordinator和 tf.QueueRunner，从设计上两个类必须被一起使用。</p>
<ul>
<li>Coordinator类用来同时停止多个工作线程并且向那个在等待所有工作线程终止的程序报告异常。</li>
<li>QueueRunner类用来协调多个工作线程同时将多个tensor压入同一个队列中。</li>
</ul>
<h4 id="读二进制文件数据到队列中"><a href="#读二进制文件数据到队列中" class="headerlink" title="读二进制文件数据到队列中"></a>读二进制文件数据到队列中</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"> </div><div class="line">def read_and_decode_single_example(filename_queue):	<span class="comment">#输入参数：a queue for an input pipeline</span></div><div class="line">    class Image(self):					<span class="comment"># 定义一个空的类对象，类似于c语言里面的结构体定义</span></div><div class="line">    pass						<span class="comment"># pass是空语句，不做任何事情，为了保持程序结构的完整性,一般用做占位语句。</span></div><div class="line">    image = Image()</div><div class="line">    image.height = 32</div><div class="line">    image.width = 32</div><div class="line">    image.depth = 3</div><div class="line">    label_bytes = 1</div><div class="line">     </div><div class="line">    <span class="comment"># compute record bytes</span></div><div class="line">    Bytes_to_read = label_bytes + image.heigth * image.width * 3</div><div class="line">    <span class="comment"># A Reader that outputs fixed-length records from a file</span></div><div class="line">    reader = tf.FixedLengthRecordReader(record_bytes=Bytes_to_read) </div><div class="line">    <span class="comment"># Returns the next record (key, value) pair produced by a reader, key 和value都是字符串类型的tensor</span></div><div class="line">    <span class="comment"># Will dequeue a work unit from queue if necessary (e.g. when the</span></div><div class="line">    <span class="comment"># Reader needs to start reading from a new file since it has</span></div><div class="line">    <span class="comment"># finished with the previous file).</span></div><div class="line">    image.key, value_str = reader.read(filename_queue) </div><div class="line">    <span class="comment"># Reinterpret the bytes of a string as a vector of numbers,每一个数值占用一个字节,在[0, 255]区间内，因此out_type要取uint8类型</span></div><div class="line">    value = tf.decode_raw(bytes=value_str, out_type=tf.uint8) </div><div class="line">    <span class="comment"># Extracts a slice from a tensor， value中包含了label和feature，故要对向量类型tensor进行'parse'操作</span></div><div class="line">    image.label = tf.slice(input_=value, begin=[0], size=[1])</div><div class="line">    value = value.slice(input_=value, begin=[1], size=[-1]).reshape((image.depth, image.height, image.width))</div><div class="line">    transposed_value = tf.transpose(value, perm=[2, 0, 1]) </div><div class="line">    image.mat = transposed_value </div><div class="line">    <span class="built_in">return</span> image</div></pre></td></tr></table></figure>
<h5 id="函数调用"><a href="#函数调用" class="headerlink" title="函数调用"></a>函数调用</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">filenames =[os.path.join(data_dir, <span class="string">'test_batch.bin'</span>)]		<span class="comment">#拼接路径,可以根据系统自动选择正确的路径分隔符"/"或"\"</span></div><div class="line"><span class="comment"># Output strings (e.g. filenames) to a queue for an input pipeline</span></div><div class="line">filename_queue = tf.train.string_input_producer(string_tensor=filenames) 	<span class="comment">#将filenames转为queue</span></div><div class="line"><span class="comment"># returns symbolic label and image</span></div><div class="line">img_obj = read_and_decode_single_example(<span class="string">"filename_queue"</span>)</div><div class="line">Label = img_obj.label</div><div class="line">Image = img_obj.mat</div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 初始化tensorflow图中的所有状态，如待读取的下一个记录tfrecord的位置，variables等</span></div><div class="line">init = tf.initialize_all_variables()</div><div class="line">sess.run(init)</div><div class="line">tf.train.start_queue_runners(sess=sess)</div><div class="line"><span class="comment"># grab examples back.</span></div><div class="line"><span class="comment"># first example from file</span></div><div class="line">label_val_1, image_val_1 = sess.run([label, image])</div><div class="line"><span class="comment"># second example from file</span></div><div class="line">label_val_2, image_val_2 = sess.run([label, image])</div></pre></td></tr></table></figure>
<p>tf.train.string_input_producer：<br>创建一个QueueRunners线程，添加QueueRunner到数据流图中。</p>
<p>tf.train.start_queue_runners(sess=sess)：<br>在运行任何训练步骤之前，要告知tensorflow去启动这些线程，否则这些队列会因为等待数据入队而被堵塞，导致数据流图将一直处于挂起状态。可以调用tf.train.start_queue_runners(sess=sess)来启动所有的QueueRunners。<br>这个调用并不是符号化的操作，它会启动输入管道的线程，填充样本到队列中，以便出队操作可以从队列中拿到样本。<br>另外，必须要先运行初始化操作再创建这些线程。如果这些队列未被初始化，tensorflow会抛出错误。</p>
<h4 id="从二进制文件中读取mini-batchs"><a href="#从二进制文件中读取mini-batchs" class="headerlink" title="从二进制文件中读取mini-batchs"></a>从二进制文件中读取mini-batchs</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从队列中随机筛选多个样例返回给image_batch和label_batch）</span></div><div class="line">image_batch, label_batch = tf.train.shuffle_batch(tensor_list=[image, label]], </div><div class="line">                                                  batch_size=batch_size, </div><div class="line">                                                  num_threads=24, </div><div class="line">                                                  min_after_dequeue=min_samples_in_queue,</div><div class="line">                                                  capacity=min_samples_in_queue+3*batch_size)</div></pre></td></tr></table></figure>
<p>读取batch数据需要使用新的队列queues和QueueRunners（大致流程图如下）。</p>
<p>Shuffle_batch:<br>构建一个RandomShuffleQueue，并不断地把单个的（image，labels）对送入队列中，入队操作是通过QueueRunners启动另外的线程来完成的。<br>这个RandomShuffleQueue会顺序地压样例到队列中，直到队列中的样例个数达到了batch_size+min_after_dequeue个。然后从队列中选择batch_size个随机的元素进行返回。<br>事实上，shuffle_batch返回的值就是RandomShuffleQueue.dequeue_many()的结果。有了这个batches变量，就可以开始训练机器学习模型了。</p>
<h5 id="tf-train-shuffle-batch-tensor-list-batch-size-capacity-min-after-dequeue-num-threads-1-seed-None-enqueue-many-False-shapes-None-shared-name-None-name-None-参数说明："><a href="#tf-train-shuffle-batch-tensor-list-batch-size-capacity-min-after-dequeue-num-threads-1-seed-None-enqueue-many-False-shapes-None-shared-name-None-name-None-参数说明：" class="headerlink" title="tf.train.shuffle_batch(tensor_list, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, shared_name=None, name=None)参数说明："></a>tf.train.shuffle_batch(tensor_list, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, shared_name=None, name=None)参数说明：</h5><p>tensor_list： 待入队的tensor list；<br>batch_size：训练时用batch_size；<br>capacity：队列长度，整型；<br>min_after_dequeue：随机取样的样本总体最小值，用于保证所取mini-batch的随机性；<br>num_threads：session会话支持多线程，这里可以设置多线程加速样本的读取）<br>seed: Seed for the random shuffling within the queue.<br>enqueue_many：为False时表示tensor_list是一个样例，压入时占用队列中的一个元素；为True时表示tensor_list中的每一个元素都是一个样例，压入时占用队列中的一个元素位置，可以看作为一个batch；<br>shapes: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensor_list</code>.<br>shared_name: (Optional) If set, this queue will be shared under the given name across multiple sessions.<br>name: (Optional) A name for the operations.</p>
<h5 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h5><p><a href="http://blog.csdn.net/diligent_321/article/details/53008840" target="_blank" rel="external"> TensorFlow读取二进制文件数据到队列</a></p>

      

      
        
    </div>
  </div>
  
</article>


  
    <article id="post-Tensorflow vs Caffe" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/03/24/Tensorflow vs Caffe/">inception_v2性能对比</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/24/Tensorflow vs Caffe/" class="article-date">
  <time datetime="2017-03-24T14:39:00.000Z" itemprop="datePublished">2017-03-24</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="fine-tune-on-flowers-datasets-arguments-in-Tensorflow"><a href="#fine-tune-on-flowers-datasets-arguments-in-Tensorflow" class="headerlink" title="fine-tune on flowers datasets arguments in Tensorflow"></a>fine-tune on flowers datasets arguments in Tensorflow</h4><p>  –max_number_of_steps=100000 \<br>  –batch_size=32 \<br>  –learning_rate=0.01 \<br>  –learning_rate_decay_type=fixed \<br>  –save_interval_secs=60 \<br>  –save_summaries_secs=60 \<br>  –log_every_n_steps=100 \<br>  –optimizer=rmsprop \<br>  –weight_decay=0.00004</p>
<p>测试结果：</p>
<ul>
<li>GPU memory：7829M</li>
<li>speed： 0.09 sec/step    </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Fine-tune only the new layers for 91600 steps</span></div><div class="line">...</div><div class="line">INFO:tensorflow:global step 91000: loss = 0.3561 (0.09 sec/step)</div><div class="line">INFO:tensorflow:global step 91100: loss = 0.3328 (0.10 sec/step)</div><div class="line">INFO:tensorflow:global step 91200: loss = 0.5208 (0.10 sec/step)</div><div class="line">INFO:tensorflow:global step 91300: loss = 0.4623 (0.09 sec/step)</div><div class="line">INFO:tensorflow:global step 91400: loss = 0.9602 (0.10 sec/step)</div><div class="line">INFO:tensorflow:global step 91500: loss = 0.8571 (0.10 sec/step)</div><div class="line">INFO:tensorflow:global step 91600: loss = 0.6486 (0.10 sec/step)</div><div class="line">...</div></pre></td></tr></table></figure>
<pre><code class="bash"><span class="comment"># evaluation</span>
...
I tensorflow/core/kernels/logging_ops.cc:79] <span class="built_in">eval</span>/Accuracy[0.8875]
I tensorflow/core/kernels/logging_ops.cc:79] <span class="built_in">eval</span>/Recall_5[1]
</code></pre>
<h4 id="fine-tune-on-cloth-datasets-arguments-in-Caffe"><a href="#fine-tune-on-cloth-datasets-arguments-in-Caffe" class="headerlink" title="fine-tune on cloth datasets arguments in Caffe"></a>fine-tune on cloth datasets arguments in Caffe</h4><p>display: 100<br>average_loss: 40<br>base_lr: 0.01<br>lr_policy: “step”<br>stepsize: 200000<br>gamma: 0.1<br>max_iter:1000000<br>momentum: 0.9<br>weight_decay: 0.00004<br>snapshot: 1000</p>
<p>测试结果：</p>
<ul>
<li>GPU memory：6047M</li>
<li>speed：0.58 sec/step</li>
</ul>
<h5 id="尚未在tensorflow上调试通过-fine-tune-on-cloth-datasets-…"><a href="#尚未在tensorflow上调试通过-fine-tune-on-cloth-datasets-…" class="headerlink" title="尚未在tensorflow上调试通过 fine-tune on cloth datasets …"></a>尚未在tensorflow上调试通过 fine-tune on cloth datasets …</h5>
      

      
        
    </div>
  </div>
  
</article>


  
    <article id="post-Tensorflow TFRecord" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/03/24/Tensorflow TFRecord/">Tensorflow Queue</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/24/Tensorflow TFRecord/" class="article-date">
  <time datetime="2017-03-24T14:38:22.000Z" itemprop="datePublished">2017-03-24</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>TensorFlow有它自己的二进制格式，使用a mixture of its Records 格式和protobuf。<br>Protobuf是一种序列化数据结构的方式，给出了关于数据的一些描述。<br>TFRecords是tensorflow的默认数据格式，一个record就是一个包含了序列化tf.train.Example 协议缓存对象的二进制文件，可以使用python创建这种格式，然后便可以使用tensorflow提供的函数来输入给机器学习模型。</p>
<h3 id="Tensorflow支持多种样例输入的方式"><a href="#Tensorflow支持多种样例输入的方式" class="headerlink" title="Tensorflow支持多种样例输入的方式"></a>Tensorflow支持多种样例输入的方式</h3><ul>
<li>placeholder：较易使用，但这需要手动传递numpy.array类型的数据。</li>
<li>队列机制：使用二进制文件和输入队列的组合形式进行异步计算。<br>这种方式不仅可以节省代码量，避免data augmentation和读文件操作，处理不同类型的数据， 而且也不再需要人为地划分开“预处理”和“模型计算”。</li>
</ul>
<h4 id="生成TFRecord"><a href="#生成TFRecord" class="headerlink" title="生成TFRecord"></a>生成TFRecord</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import tensorflow as tf </div><div class="line">from PIL import Image</div><div class="line">import sys</div><div class="line"></div><div class="line"><span class="comment"># 参数1：各分类图片所在目录</span></div><div class="line">imgPath = sys.argv[1]				</div><div class="line">classes = os.listdir(imgPath)					<span class="comment">#各分类图片所在目录存储在classes</span></div><div class="line"><span class="comment"># </span></div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init_op)</div><div class="line"><span class="comment"># 参数2：保存TFRecord文件目录</span></div><div class="line">svPath = sys.argv[2]				</div><div class="line">writer = tf.python_io.TFRecordWriter(svPath)	<span class="comment">#tf.python_io.TFRecordWriter进行写入</span></div><div class="line"><span class="comment"># enumerate既遍历索引又遍历元素，index存储索引，下面会记录为图片的label， name存储元素</span></div><div class="line"><span class="keyword">for</span> index, name <span class="keyword">in</span> enumerate(classes):		</div><div class="line">    class_path = imgPath + name + <span class="string">"/"</span></div><div class="line">    <span class="comment"># os.listdir()函数获得指定目录中的内容， img_name列举每张图片名称</span></div><div class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> os.listdir(class_path):	</div><div class="line">	    img_path = class_path + img_name		<span class="comment">#img_path存储每张图片的“绝对”路径</span></div><div class="line">        img = Image.open(img_path)</div><div class="line">        img = img.resize((224, 224))</div><div class="line">		<span class="comment">#img.save("tempResize.jpg")				#存储图片进行检验</span></div><div class="line">	    img_raw = img.tobytes()          		<span class="comment">#将图片转化为原生bytes</span></div><div class="line">		<span class="comment">#使用tf.train.Example来定义填入的数据格式</span></div><div class="line">        example = tf.train.Example(features=tf.train.Features(feature=&#123;</div><div class="line">            <span class="string">"label"</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),</div><div class="line">            <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))</div><div class="line">       &#125;))</div><div class="line">        writer.write(example.SerializeToString())  <span class="comment">#序列化为字符串</span></div><div class="line">writer.close()</div></pre></td></tr></table></figure>
<p>一个Example中包含Features，Features里包含Feature（没有s）的字典，Feature里包含有一个 FloatList， 或者ByteList，或者Int64List。</p>
<h4 id="读取TFRecord"><a href="#读取TFRecord" class="headerlink" title="读取TFRecord"></a>读取TFRecord</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 此处各变量沿用上述定义</span></div><div class="line"><span class="keyword">for</span> serialized_example <span class="keyword">in</span> tf.python_io.tf_record_iterator(svPath):</div><div class="line">    example = tf.train.Example()</div><div class="line">    example.ParseFromString(serialized_example)		<span class="comment">#解析字符串</span></div><div class="line">	image = example.features.feature[<span class="string">'image'</span>].bytes_list.value</div><div class="line">    label = example.features.feature[<span class="string">'label'</span>].int64_list.value</div><div class="line">    <span class="comment"># </span></div><div class="line">    <span class="built_in">print</span> image, label</div></pre></td></tr></table></figure>
<h4 id="使用队列（queue）读取TFRecord"><a href="#使用队列（queue）读取TFRecord" class="headerlink" title="使用队列（queue）读取TFRecord"></a>使用队列（queue）读取TFRecord</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def read_and_decode(filename):</div><div class="line">    <span class="comment">#根据文件名生成一个队列,即将filenames（即为上述生成的tfrecord文件）转为queue</span></div><div class="line">    filename_queue = tf.train.string_input_producer([filename])		</div><div class="line">    reader = tf.TFRecordReader()</div><div class="line">    _, serialized_example = reader.read(filename_queue)</div><div class="line">    <span class="comment"># parse_single_example解释器从TFRecords文件中读取数据，将Example协议内存块(protocol buffer)解析为张量。</span></div><div class="line">    features = tf.parse_single_example(serialized_example,</div><div class="line">                                       features=&#123;</div><div class="line">                                           <span class="string">'label'</span>: tf.FixedLenFeature([], tf.int64),</div><div class="line">                                           <span class="string">'img_raw'</span> : tf.FixedLenFeature([], tf.string),</div><div class="line">                                       &#125;)</div><div class="line">    img = tf.decode_raw(features[<span class="string">'img_raw'</span>], tf.uint8)			<span class="comment">#解析img_raw，并将其转换为uint8向量</span></div><div class="line">	img = tf.reshape(img, [224, 224, 3])						<span class="comment">#reshape，将tensor转为参数shape形式</span></div><div class="line">    img = tf.cast(img, tf.float32) * (1. / 255) - 0.5			<span class="comment">#归一化数据到[-0.5, 0.5]</span></div><div class="line">    label = tf.cast(features[<span class="string">'label'</span>], tf.int32)				<span class="comment">#将label从uint8转换为int32</span></div><div class="line">    <span class="built_in">return</span> img, label</div></pre></td></tr></table></figure>
<p>TFRecordReader作用于文件名队列。它从队列中弹出文件名并使用该文件名，直到tfrecord为空时停止，此时它将从文件名队列中弹出下一个filename。<br>参数shape为一个列表形式，特殊的一点是列表中可以存在-1。-1代表的含义是不用我们自己指定这一维的大小，函数会自动计算，但列表中只能存在一个-1（当然如果存在多个-1，就是一个存在多解的方程了）。</p>
<ul>
<li>如果 shape=[-1], 表示要将tensor展开成一个list </li>
<li>如果 shape=[a,b,c,…] 其中每个a,b,c,..均&gt;0，那么就是常规用法 </li>
<li>如果 shape=[a,-1,c,…] 此时b=-1，a,c,..依然&gt;0。这表示tf会根据tensor的原尺寸，自动计算b的值。 </li>
</ul>
<h5 id="训练调用read-and-decode"><a href="#训练调用read-and-decode" class="headerlink" title="训练调用read_and_decode"></a>训练调用read_and_decode</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 此处各变量沿用上述定义</span></div><div class="line">img, label = read_and_decode(svPath)</div><div class="line"><span class="comment"># 使用shuffle_batch随机打乱输入</span></div><div class="line">img_batch, label_batch = tf.train.shuffle_batch([img, label],</div><div class="line">                                                batch_size=30, capacity=2000,</div><div class="line">                                                min_after_dequeue=1000)</div><div class="line">init = tf.initialize_all_variables()			<span class="comment">#初始化整个graph，如待读取的下一个记录tfrecord的位置，variables等</span></div><div class="line">with tf.Session() as sess:						<span class="comment">#创建Session</span></div><div class="line">    sess.run(init)</div><div class="line">    threads = tf.train.start_queue_runners(sess=sess)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(3):</div><div class="line">        val, l= sess.run([img_batch, label_batch])</div><div class="line">        <span class="comment">#我们也可以根据需要对val， l进行处理</span></div><div class="line">        <span class="comment">#l = to_categorical(l, 12) </span></div><div class="line">        <span class="built_in">print</span>(val.shape, l)</div></pre></td></tr></table></figure>
<p>关于队列的详解将会在下篇博客中给出。。。</p>
<h5 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h5><p><a href="http://ycszen.github.io/2016/08/17/TensorFlow%E9%AB%98%E6%95%88%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/" target="_blank" rel="external">TensorFlow高效读取数据</a></p>

      

      
        
    </div>
  </div>
  
</article>


  
    <article id="post-TensorFlow-tips笔记-1" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/03/23/TensorFlow-tips笔记-1/">TensorFlow tips笔记-1</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/23/TensorFlow-tips笔记-1/" class="article-date">
  <time datetime="2017-03-23T10:53:32.000Z" itemprop="datePublished">2017-03-23</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><ul>
<li>TensorFlow是一种将计算表示为图的编程系统。图中的节点称为ops(operation的简称)。一个ops使用0个或以上的Tensors，通过执行某些运算，产生0个或以上的Tensors。</li>
<li>TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPUs和GPUs)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor.</li>
<li>图的创建类似于一个 [施工阶段]，而在 [执行阶段] 则利用一个session来执行图中的节点。很常见的情况是，在 [施工阶段] 创建一个图来表示和训练神经网络，而在 [执行阶段] 在图中重复执行一系列的训练操作。</li>
<li>Constant是一种没有输入的ops，但是你可以将它作为其他ops的输入。Python库中的ops构造器将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点。</li>
</ul>
<h3 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h3><p>1、graph(图)：<br>即计算任务</p>
<p>2、op(operation缩写)：<br>一个完成任务的步骤</p>
<p>3、session(会话)：<br>实现图和计算内核的交互。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ python                          		  <span class="comment">#进入python语言环境,出现&gt;&gt;&gt;即代表进入了</span></div><div class="line">&gt;&gt;&gt; import tensorflow as tf       		  <span class="comment">#1,声明引用tensorflow，在后面的代码中以tf来使用tensorflow的所有功能</span></div><div class="line">&gt;&gt;&gt; hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)	  <span class="comment">#2,创建一个变量hello，其值是tf的一个op结果</span></div><div class="line">&gt;&gt;&gt; sess = tf.Session()           		  <span class="comment">#3,创建一个变量sess，其是tf的一个Session实例</span></div><div class="line">&gt;&gt;&gt; <span class="built_in">print</span> hello</div><div class="line">Tensor(<span class="string">"Const:0"</span>, shape=(), dtype=string)	  <span class="comment">#只是类型说明，不是具体值</span></div><div class="line">&gt;&gt;&gt; <span class="built_in">print</span> sess.run(hello)         		  <span class="comment">#4,调用Session对象的run方法对hello这个op进行处理，然后将结果返回给print打印</span></div><div class="line">Hello, TensorFlow!</div><div class="line">&gt;&gt;&gt; a = tf.constant(10)       			  <span class="comment">#5, 创建一个变量a，其值是tf的一个op结果</span></div><div class="line">&gt;&gt;&gt; b = tf.constant(32)       			  <span class="comment">#6, 创建一个变量b，其值是tf的一个op结果</span></div><div class="line">&gt;&gt;&gt; <span class="built_in">print</span> a </div><div class="line">Tensor(<span class="string">"Const_1:0"</span>, shape=(), dtype=int32)	<span class="comment">#只是类型说明，不是具体值</span></div><div class="line">&gt;&gt;&gt; <span class="built_in">print</span> b</div><div class="line">Tensor(<span class="string">"Const_2:0"</span>, shape=(), dtype=int32)	<span class="comment">#只是类型说明，不是具体值</span></div><div class="line">&gt;&gt;&gt; <span class="built_in">print</span> sess.run(a+b)             		<span class="comment">#7, 调用Session对象的run方法对a这个op和b这个op进行加法处理，然后将结果返回给print打印</span></div><div class="line">42</div></pre></td></tr></table></figure>
<ul>
<li>“TensorFlow程序通常被组织成一个构建阶段, 和一个执行阶段. 在构建阶段, op 的执行步骤被描述成一个图.在执行阶段, 使用会话执行图中的 op.”</li>
<li>在tensorflow中，调用run方法时是执行阶段，其它是构建阶段，构建阶段你声明和定义的任何op操作只是被记下来了，但并没有落实，只有到了run时，tensorflow才真正的使用CPU执行这些op操作。这样基本上消除了一段代码内在tf和python之间的切换，会大大减小开支，提升计算效率。同时，将很多操作攒在一堆，也方便分布式执行。</li>
<li>会话 (Session)存在的意义就是为了区别执行阶段和构建阶段，因为常规开发语言中，每一行代码都是执行阶段。</li>
<li>Sessions最后需要关闭，以释放相关的资源；你也可以使用with模块，session在with模块中自动会关闭。</li>
<li>Session是Graph和执行者之间的媒介，Session.run()实际上将graph、fetches、feed_dict序列化到字节数组中，并调用tf_session.TF_Run（参见/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py）</li>
<li>而这里的tf_session.TF_Run实际上调用了动态链接库_pywrap_tensorflow.so中实现的_pywrap_tensorflow.TF_Run接口(参见/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py)，这个动态链接库是tensorflow提供的诸多语言接口中python语言的接口；</li>
<li>事实上这里的_pywrap_tensorflow.so和pywrap_tensorflow.py是通过SWIG工具自动生成，大家都知道tensorflow核心语言是c语言，这里是通过SWIG生成了各种脚本语言的接口。</li>
</ul>
<p>4、Tensor<br>用来表示常规的程序数据。</p>
<p>每个 Tensor 是一个类型化的多维数组，一个Tensor具有固定的类型、级别和大小。<br>一个 op获得 0 个或多个Tensor , 执行计算, 产生 0 个或多个Tensor 。</p>
<p>5、tensorboard</p>
<p>tensorflow因为代码执行过程是先构建图，然后在执行，对中间过程的调试不太方便，所以提供了一个tensorboard工具来便于调试，用法如下：</p>
<p>在训练时会提示写入事件文件到哪个目录(比如：/tmp/tflearn_logs/11U8M4/)<br>执行如下命令并打开<a href="http://192.168.1.101:6006就能看到tensorboard的界面：" target="_blank" rel="external">http://192.168.1.101:6006就能看到tensorboard的界面：</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=/tmp/tflearn_logs/11U8M4/</div></pre></td></tr></table></figure></p>
<p>生成的图就是上面这段代码生成的graph结构，这个graph描述了整个梯度下降解决线性回归问题的整个过程，每一个节点都代表了代码中的一步操作。</p>
<p>6、Variables</p>
<p>变量在图执行的过程中，保持自己的状态信息。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a Variable, that will be initialized to the scalar value 0.</span></div><div class="line">state = tf.Variable(0, name=<span class="string">"counter"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create an Op to add one to `state`.</span></div><div class="line"></div><div class="line">one = tf.constant(1)</div><div class="line">new_value = tf.add(state, one)</div><div class="line">update = tf.assign(state, new_value)</div></pre></td></tr></table></figure>
<p>所有tensors的输出都是一次性连贯执行的。</p>
<p>7、placeholder占位符</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">input1 = tf.placeholder(tf.float32)</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line">output = tf.mul(input1, input2)</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">  <span class="built_in">print</span>(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;))</div></pre></td></tr></table></figure>
<h3 id="技术支持"><a href="#技术支持" class="headerlink" title="技术支持"></a>技术支持</h3><p>1、Tensorflow是完全采用了符号式编程，每个节点都是符号化表示的。通过session创建graph，在调用session.run执行计算。</p>
<ul>
<li>符号式编程将计算过程抽象为计算图，计算流图可以方便的描述计算过程，所有输入节点、运算节点、输出节点均符号化处理。</li>
<li>计算图通过建立输入节点到输出节点的传递闭包，从输入节点出发，沿着传递闭包完成数值计算和数据流动，直到达到输出节点。</li>
<li>这个过程经过计算图优化，以数据（计算）流方式完成，节省内存空间使用，计算速度快，但不适合程序调试，通常不用于编程语言中。</li>
</ul>
<p>2、TF最大的特点是强化了数据流图，引入了mutation的概念。</p>
<ul>
<li>TF的计算图如同数据流一样，数据流向表示计算过程。这一点是TF和包括Theano在内的符号编程框架最大的不同。</li>
<li>所谓mutation，就是可以在计算的过程更改一个变量的值，而这个变量在计算的过程中会被带入到下一轮迭代里面去。</li>
<li>Mutation是机器学习优化算法几乎必须要引入的东西，TF选择了纯符号计算的路线，并且直接把更新引入了数据流图中去。</li>
</ul>
<p>3、梯度计算</p>
<ul>
<li>梯度计算涉及每个计算节点，每个自定义的前向计算图都包含一个隐式的反向计算图。</li>
<li>从数据流向上看，正向计算图是数据从输入节点到输出节点的流向过程，反向计算图是数据从输出节点到输入节点的流向过程。</li>
<li>反向计算限制了符号编程中内存空间复用的优势，因为在正向计算中的计算数据在反向计算中也可能要用到。</li>
</ul>
<p>4、控制流</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.cond(pred,fn1,fn2,name=None) <span class="comment">#pred为判别表达式，fn1和fn2为运算表达式。当pred为true是，执行fn1操作；当pred为false时，执行fn2操作。</span></div><div class="line">tf.control_dependencies(control_inputs) <span class="comment">#运算控制器，控制多个数据流执行完成后才能执行接下来的操作，通常与tf.group函数结合使用</span></div></pre></td></tr></table></figure>
<p>TF不仅支持逻辑控制，还支持循环控制。将循环的每次迭代标记为一个tag，迭代的执行状态标记为一个frame，当迭代所需的数据准备好的时候，就可以开始计算，从而多个迭代可以同时执行。</p>
<p>5、使用Eigen库</p>
<ul>
<li>在Tensoflow中核心数据结构和运算主要依赖于Eigen和Stream Executor库，其中Eigen支持CPU和GPU加速计算，Stream Executor主要用于GPU环境加速计算。</li>
<li>Eigen是高效易用的C++开源库，有效支持线性代数，矩阵和矢量运算，数值分析及其相关的算法。不依赖于任何其他依赖包，安装使用都很简便。<br>Eigen 惰性求值也认为是“延迟求值”，不需要临时变量保存中间结果，可以构造一个无限的数据类型，提高了计算性能。</li>
<li>Eigen 编译加速使用了SSE2加速。假设处理float32类型，指令集支持128bit并行计算，则一次可以计算4个float32类型，速度提升4倍。可以充分发挥计算机的并行计算能力，提高程序运行速度。</li>
<li>Eigen::half，Tensorflow支持的浮点数类型有float16, float32, float64，其中float16本质上是Eigen::half类型，即半精度浮点数。在Tensorflow的分布式传输中，默认会将float32转换为float16类型。</li>
</ul>
<p>6、TF设备内存管理模块利用BFC算法（best-fit with coalescing）实现。</p>
<ul>
<li>BFC算法是Doung Lea’s malloc(dlmalloc)的一个非常简单的版本。它具有内存分配、释放、碎片管理等基本功能。</li>
<li>BFC的核心思想是：将内存分块管理，按块进行空间分配和释放；通过split操作将大内存块分解成小内存块；通过merge操作合并小的内存块，做到内存碎片回收。</li>
</ul>
<p>7、TF系统开发使用了bazel工具实现工程代码自动化管理，使用了protobuf实现了跨设备数据传输，使用了swig库实现python接口封装。</p>
<ul>
<li>TF中几乎所有代码编译生成都是依赖Bazel完成的，Bazel是Google开源的自动化构建工具，类似于Make和CMake工具。</li>
<li>Bazel的目标是构建“快速并可靠的代码”，并且能“随着公司的成长持续调整其软件开发实践”。</li>
<li>Bazel假定每个目录为[package]单元，目录里面包含了源文件和一个描述文件BUILD，描述文件中指定了如何将源文件转换成构建的输出。</li>
<li>Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。</li>
<li>Protobuf对象描述文件为.proto类型，编译后生成.pb.h和.<a href="http://pb.cc文件。" target="_blank" rel="external">http://pb.cc文件。</a></li>
<li>在分布式环境中，不仅需要传输数据序列化，还需要数据传输协议。Protobuf在序列化处理后，由gRPC完成数据传输。</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="http://www.jeyzhang.com/tensorflow-learning-notes.html" target="_blank" rel="external">TensorFlow学习笔记1：入门</a></p>
<p><a href="http://www.shareditor.com/blogshow/?blogId=120" target="_blank" rel="external">自己动手做聊天机器人 三十七-一张图了解tensorflow中的线性回归工作原理</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25646408" target="_blank" rel="external">『深度长文』Tensorflow代码解析（一）</a></p>
<p><a href="http://hacker.duanshishi.com/?p=1639" target="_blank" rel="external">TensorFlow入门一</a></p>

      

      
        
    </div>
  </div>
  
</article>


  
    <article id="post-深度学习tips总结" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/03/23/深度学习tips总结/">深度学习tips总结</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/23/深度学习tips总结/" class="article-date">
  <time datetime="2017-03-23T02:01:16.000Z" itemprop="datePublished">2017-03-23</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>注：本文是在阅读<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2651994342&amp;idx=5&amp;sn=dbae830cebb360f78f43191cf5d2c7ab" target="_blank" rel="external">新智元的文章</a>后总结，文中不少语句参考了原文作者，请知晓。</p>
<h3 id="1、shuffle"><a href="#1、shuffle" class="headerlink" title="1、shuffle"></a>1、shuffle</h3><p>在框架允许的前提下，每个epoch要shuffle一次;</p>
<h3 id="2、扩展数据集"><a href="#2、扩展数据集" class="headerlink" title="2、扩展数据集"></a>2、扩展数据集</h3><p>小数据集容易使得模型过拟合，但过分扩展会使得大都相同的数据，需采取一定的方法，避免出现相同的样本（尝试中）；</p>
<h3 id="3、在非常小的子数据集上训练进行过拟合，再在整个数据集上训练"><a href="#3、在非常小的子数据集上训练进行过拟合，再在整个数据集上训练" class="headerlink" title="3、在非常小的子数据集上训练进行过拟合，再在整个数据集上训练"></a>3、在非常小的子数据集上训练进行过拟合，再在整个数据集上训练</h3><p>以确定网络可以收敛；</p>
<h3 id="4、始终是用dropout将过拟合的几率最小化"><a href="#4、始终是用dropout将过拟合的几率最小化" class="headerlink" title="4、始终是用dropout将过拟合的几率最小化"></a>4、始终是用dropout将过拟合的几率最小化</h3><p>当神经元节点超过256时，就要使用dropout，Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning [Gal Yarin &amp; Zoubin Ghahramani，2015].；</p>
<h3 id="5、MAX-pooling会更快，避免使用LRN-pooling"><a href="#5、MAX-pooling会更快，避免使用LRN-pooling" class="headerlink" title="5、MAX pooling会更快，避免使用LRN pooling"></a>5、MAX pooling会更快，避免使用LRN pooling</h3><h3 id="避免使用sigmoid-tanh"><a href="#避免使用sigmoid-tanh" class="headerlink" title="避免使用sigmoid/tanh"></a>避免使用sigmoid/tanh</h3><p>代价昂贵，容易饱和，网络越深，越容易停止反向传播；而更简单有效的ReLU和PreLU能够促进稀疏性，其反向传播也更加鲁棒，Deep Sparse Rectifier Neural Networks；</p>
<h3 id="7、在max-pooling之前不要使用ReLU和PreLU，在保持计算之后再使用"><a href="#7、在max-pooling之前不要使用ReLU和PreLU，在保持计算之后再使用" class="headerlink" title="7、在max pooling之前不要使用ReLU和PreLU，在保持计算之后再使用"></a>7、在max pooling之前不要使用ReLU和PreLU，在保持计算之后再使用</h3><h3 id="8、不要使用ReLU"><a href="#8、不要使用ReLU" class="headerlink" title="8、不要使用ReLU"></a>8、不要使用ReLU</h3><p>虽然它们是很好的非线性函数，但是在微调模型时会阻碍反向传播，在初始化阶段被卡住，无法得到任何微调效果；可以使用PreLU以及一个很小的乘数（通常是0.1），收敛更快；</p>
<h3 id="9、经常使用批标准化-Batch-Normalization"><a href="#9、经常使用批标准化-Batch-Normalization" class="headerlink" title="9、经常使用批标准化(Batch Normalization)"></a>9、经常使用批标准化(Batch Normalization)</h3><p>可以允许更快的收敛以及更小的数据集，节省时间以及资源，Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [Sergey Ioffe &amp; Christian Szegedy,2015]。；</p>
<h3 id="10、相对于减去平均值，更倾向于将数据压缩到-1-1"><a href="#10、相对于减去平均值，更倾向于将数据压缩到-1-1" class="headerlink" title="10、相对于减去平均值，更倾向于将数据压缩到[-1, +1]"></a>10、相对于减去平均值，更倾向于将数据压缩到[-1, +1]</h3><p>针对训练和部署的技巧，而非提升性能；</p>
<h3 id="11、小型化模型，并尝试-ensemble"><a href="#11、小型化模型，并尝试-ensemble" class="headerlink" title="11、小型化模型，并尝试 ensemble"></a>11、小型化模型，并尝试 ensemble</h3><p>以方便用户及服务，并提升准确度；</p>
<h3 id="12、尽可能使用-xavier-初始化"><a href="#12、尽可能使用-xavier-初始化" class="headerlink" title="12、尽可能使用 xavier 初始化"></a>12、尽可能使用 xavier 初始化</h3><p>可在较大的全连接层上使用，避免在CNN层使用，An Explanation of Xavier Initialization（by Andy Jones）；</p>
<h3 id="13、如果输入数据有空间参数，可以尝试端到端CNN"><a href="#13、如果输入数据有空间参数，可以尝试端到端CNN" class="headerlink" title="13、如果输入数据有空间参数，可以尝试端到端CNN"></a>13、如果输入数据有空间参数，可以尝试端到端CNN</h3><p>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size [Forrest N. Iandola et. al. 2016];</p>
<h3 id="14、修改模型，只要可能就使用-1x1-的-CNN-层，它的位置对提高性能很有帮助。"><a href="#14、修改模型，只要可能就使用-1x1-的-CNN-层，它的位置对提高性能很有帮助。" class="headerlink" title="14、修改模型，只要可能就使用 1x1 的 CNN 层，它的位置对提高性能很有帮助。"></a>14、修改模型，只要可能就使用 1x1 的 CNN 层，它的位置对提高性能很有帮助。</h3><h3 id="15、假如你要利用模型或你自己的层来制作模板，记得把所有东西参数化，否则你得重建所有二进制文件"><a href="#15、假如你要利用模型或你自己的层来制作模板，记得把所有东西参数化，否则你得重建所有二进制文件" class="headerlink" title="15、假如你要利用模型或你自己的层来制作模板，记得把所有东西参数化，否则你得重建所有二进制文件"></a>15、假如你要利用模型或你自己的层来制作模板，记得把所有东西参数化，否则你得重建所有二进制文件</h3><h3 id="16、了解正在着手的任务及使用的结构，莫要盲目地复制模型；"><a href="#16、了解正在着手的任务及使用的结构，莫要盲目地复制模型；" class="headerlink" title="16、了解正在着手的任务及使用的结构，莫要盲目地复制模型；"></a>16、了解正在着手的任务及使用的结构，莫要盲目地复制模型；</h3><h4 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h4><p>1、 Ian Goodfellow , Yoshua Bengio 和 Aaron Courville 合著的经典教材《深度学习》（数学理论丰富）</p>
<p>2、邓力和俞栋合著的《深度学习：方法及应用》（关于历史与介绍）</p>
<p>3、Timothy Masters 写的 Deep Belief Nets in C++ and CUDA C, Vol. 1: Restricted Boltzmann Machines and Supervised Feedforward Networks（实现算法）</p>

      

      
        
    </div>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/03/05/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/05/hello-world/" class="article-date">
  <time datetime="2017-03-05T09:03:32.000Z" itemprop="datePublished">2017-03-05</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any pr/Users/bice/Documents/blog/source/_posts/newtest.mdoblems when using Hexo, you can find the answe/Users/bice/Documents/blog/source/_posts/newtest.mdr in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      

      
        
    </div>
  </div>
  
</article>


  
  
</section>
        <aside id="sidebar">
  <nav class="menus">
  	<ul>
  		<li><a href="/"><i class="icon icon-home"></i></a></li>
  		
			<li><a href="/archives"><i class="icon icon-fenlei"></i></a></li>
  		
  		
			<li><a href="/tags"><i class="icon icon-tag"></i></a></li>
  		
  		
  			<li><a href="https://github.com/" target="_blank"><i class="icon icon-github"></i></a></li>
  		
  	</ul>
  </nav>
  <a id="go-top" href="#"><i class="icon icon-up"></i></a>
</aside>
      </div>
      <footer id="footer">
  
	<div id="footer-info" class="inner">
	  &copy; 2017 XiaoShuang He 
	  - Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	  - Theme <a href="https://github.com/hejianxian/hexo-theme-jane/" target="_blank">Jane</a>
	</div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/tags" class="mobile-nav-link">Tag</a>
  
    <a href="https://github.com/" class="mobile-nav-link">Github</a>
  
</nav>
    

<script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>